<!DOCTYPE html><html lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0" />
    <title>activateFunction</title>
    <meta name="author" content="xie.guigang@gcmodeller.org" />
    <meta name="copyright" content="SMRUCC genomics Copyright (c) 2022" />
    <meta name="keywords" content="R#; activateFunction; enigma" />
    <meta name="generator" content="https://github.com/rsharp-lang" />
    <meta name="theme-color" content="#333" />
    <meta name="description" content="the activate function handler An activation function is a functi..." />
    <meta class="foundation-data-attribute-namespace" />
    <meta class="foundation-mq-xxlarge" />
    <meta class="foundation-mq-xlarge" />
    <meta class="foundation-mq-large" />
    <meta class="foundation-mq-medium" />
    <meta class="foundation-mq-small" />
    <meta class="foundation-mq-topbar" />
  </head>
  <body>
    <table width="100%" summary="page for {activateFunction}">
      <tbody>
        <tr>
          <td>{activateFunction}</td>
          <td style="text-align: right;">R# Documentation</td>
        </tr>
      </tbody>
    </table>
    <h1>activateFunction</h1>
    <hr />
    <p>
      <code>
        <span style="color: blue;">require</span>(<span style="color: black; font-weight: bold;">activateFunction</span>);
                               <br /><br /><span style="color: green;">#' the activate function handler An activation function is a function used in artificial neural
#' networks which outputs a small value for small inputs, and a
#' larger value if its inputs exceed a threshold. If the inputs
#' are large enough, the activation function "fires", otherwise it
#' does nothing. In other words, an activation function is like a
#' gate that checks that an incoming value is greater than a
#' critical number. Activation functions are useful because they add non-linearities
#' into neural networks, allowing the neural networks To learn
#' powerful operations. If the activation functions were To be removed
#' from a feedforward neural network, the entire network could be
#' re-factored To a simple linear operation Or matrix transformation
#' On its input, And it would no longer be capable Of performing
#' complex tasks such As image recognition. Well-known activation functions used in data science include the
#' rectified linear unit (ReLU) function, And the family of sigmoid
#' functions such as the logistic sigmoid function, the hyperbolic
#' tangent, And the arctangent function.</span><br /><span style="color: blue;">imports</span><span style="color: brown"> "activateFunction"</span><span style="color: blue;"> from</span><span style="color: brown"> "enigma"</span>;
                           </code>
    </p>
    <br />
    <p><p>the activate function handler</p>

<p>An activation function is a function used in artificial neural<br />
 networks which outputs a small value for small inputs, and a<br />
 larger value if its inputs exceed a threshold. If the inputs<br />
 are large enough, the activation function "fires", otherwise it<br />
 does nothing. In other words, an activation function is like a<br />
 gate that checks that an incoming value is greater than a <br />
 critical number.</p>

<p>Activation functions are useful because they add non-linearities<br />
 into neural networks, allowing the neural networks To learn <br />
 powerful operations. If the activation functions were To be removed<br />
 from a feedforward neural network, the entire network could be <br />
 re-factored To a simple linear operation Or matrix transformation<br />
 On its input, And it would no longer be capable Of performing <br />
 complex tasks such As image recognition.</p>

<p>Well-known activation functions used in data science include the <br />
 rectified linear unit (ReLU) function, And the family of sigmoid <br />
 functions such as the logistic sigmoid function, the hyperbolic<br />
 tangent, And the arctangent function.</p></p>
    <p style="font-style: italic; font-size: 0.9em;"><p>Activation functions in computer science are inspired by the <br />
 action potential in neuroscience. If the electrical potential<br />
 between a neuron's interior and exterior exceeds a value called <br />
 the action potential, the neuron undergoes a chain reaction <br />
 which allows it to 'fire' and transmit a signal to neighboring <br />
 neurons. The resultant sequence of activations, called a 'spike <br />
 train', enables sensory neurons to transmit feeling from the <br />
 fingers to the brain, and allows motor neurons to transmit <br />
 instructions from the brain to the limbs.</p></p>
    <div id="main-wrapper">
      <table>
        <tbody><tr>
  <td id="sigmoid">
    <a href="./activateFunction/sigmoid.html">sigmoid</a>
  </td>
  <td><p>Logistic Sigmoid Function Formula</p></td>
</tr>
<tr>
  <td id="identical">
    <a href="./activateFunction/identical.html">identical</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="qlinear">
    <a href="./activateFunction/qlinear.html">qlinear</a>
  </td>
  <td></td>
</tr>
<tr>
  <td id="func">
    <a href="./activateFunction/func.html">func</a>
  </td>
  <td><p>create a new custom activate function</p></td>
</tr></tbody>
      </table>
    </div>
  </body>
</html>